{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNXTmN4s1MVH8yX0HaIlHFs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rgspatial/Practicing/blob/main/Logistic_Regression_Gradient_Descent_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ChsPdfF3PbZ"
      },
      "outputs": [],
      "source": [
        "import copy, math\n",
        "import numpy as np\n",
        "%matplotlib widget\n",
        "import matplotlib.pyplot as plt\n",
        "from lab_utils_common import  dlc, plot_data, plt_tumor_data, sigmoid, compute_cost_logistic\n",
        "from plt_quad_logistic import plt_quad_logistic, plt_prob\n",
        "plt.style.use('./deeplearning.mplstyle')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data set\n",
        "Let's start with the same two feature data set used in the decision boundary lab."
      ],
      "metadata": {
        "id": "-QOLKgTB--DQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\n",
        "y_train = np.array([0, 0, 0, 1, 1, 1])"
      ],
      "metadata": {
        "id": "dp7FakS9-_OI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As before, we'll use a helper function to plot this data. The data points with label  𝑦=1  are shown as red crosses, while the data points with label  𝑦=0  are shown as blue circles."
      ],
      "metadata": {
        "id": "azLQTisT_BlU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig,ax = plt.subplots(1,1,figsize=(4,4))\n",
        "plot_data(X_train, y_train, ax)\n",
        "\n",
        "ax.axis([0, 4, 0, 3.5])\n",
        "ax.set_ylabel('$x_1$', fontsize=12)\n",
        "ax.set_xlabel('$x_0$', fontsize=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "J3-ezTQs_CgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Gradient Descent\n",
        "\n",
        "Recall the gradient descent algorithm utilizes the gradient calculation:\n",
        "repeat until convergence:{𝑤𝑗=𝑤𝑗−𝛼∂𝐽(𝐰,𝑏)∂𝑤𝑗𝑏=𝑏−𝛼∂𝐽(𝐰,𝑏)∂𝑏}for j := 0..n-1(1)\n",
        "Where each iteration performs simultaneous updates on  𝑤𝑗  for all  𝑗 , where\n",
        "∂𝐽(𝐰,𝑏)∂𝑤𝑗∂𝐽(𝐰,𝑏)∂𝑏=1𝑚∑𝑖=0𝑚−1(𝑓𝐰,𝑏(𝐱(𝑖))−𝑦(𝑖))𝑥(𝑖)𝑗=1𝑚∑𝑖=0𝑚−1(𝑓𝐰,𝑏(𝐱(𝑖))−𝑦(𝑖))(2)(3)\n",
        "m is the number of training examples in the data set\n",
        "𝑓𝐰,𝑏(𝑥(𝑖))  is the model's prediction, while  𝑦(𝑖)  is the target\n",
        "For a logistic regression model\n",
        "𝑧=𝐰⋅𝐱+𝑏 \n",
        "𝑓𝐰,𝑏(𝑥)=𝑔(𝑧) \n",
        "where  𝑔(𝑧)  is the sigmoid function:\n",
        "𝑔(𝑧)=11+𝑒−𝑧 \n",
        "Gradient Descent Implementation\n",
        "The gradient descent algorithm implementation has two components:\n",
        "\n",
        "The loop implementing equation (1) above. This is gradient_descent below and is generally provided to you in optional and practice labs.\n",
        "The calculation of the current gradient, equations (2,3) above. This is compute_gradient_logistic below. You will be asked to implement this week's practice lab.\n",
        "Calculating the Gradient, Code Description\n",
        "Implements equation (2),(3) above for all 𝑤𝑗 and 𝑏. There are many ways to implement this. Outlined below is this:\n",
        "\n",
        "initialize variables to accumulate dj_dw and dj_db\n",
        "\n",
        "for each example\n",
        "\n",
        "calculate the error for that example 𝑔(𝐰⋅𝐱(𝑖)+𝑏)−𝐲(𝑖)\n",
        "for each input value 𝑥(𝑖)𝑗 in this example,\n",
        "multiply the error by the input 𝑥(𝑖)𝑗, and add to the corresponding element of dj_dw. (equation 2 above)\n",
        "add the error to dj_db (equation 3 above)\n",
        "divide dj_db and dj_dw by total number of examples (m)\n",
        "\n",
        "note that 𝐱(𝑖) in numpy X[i,:] or X[i] and 𝑥(𝑖)𝑗 is X[i,j]"
      ],
      "metadata": {
        "id": "F-Qd8FNv_WFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient_logistic(X, y, w, b): \n",
        "    \"\"\"\n",
        "    Computes the gradient for linear regression \n",
        " \n",
        "    Args:\n",
        "      X (ndarray (m,n): Data, m examples with n features\n",
        "      y (ndarray (m,)): target values\n",
        "      w (ndarray (n,)): model parameters  \n",
        "      b (scalar)      : model parameter\n",
        "    Returns\n",
        "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
        "      dj_db (scalar)      : The gradient of the cost w.r.t. the parameter b. \n",
        "    \"\"\"\n",
        "    m,n = X.shape\n",
        "    dj_dw = np.zeros((n,))                           #(n,)\n",
        "    dj_db = 0.\n",
        "\n",
        "    for i in range(m):\n",
        "        f_wb_i = sigmoid(np.dot(X[i],w) + b)          #(n,)(n,)=scalar\n",
        "        err_i  = f_wb_i  - y[i]                       #scalar\n",
        "        for j in range(n):\n",
        "            dj_dw[j] = dj_dw[j] + err_i * X[i,j]      #scalar\n",
        "        dj_db = dj_db + err_i\n",
        "    dj_dw = dj_dw/m                                   #(n,)\n",
        "    dj_db = dj_db/m                                   #scalar\n",
        "        \n",
        "    return dj_db, dj_dw  "
      ],
      "metadata": {
        "id": "P-OtezQk_Ylh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the implementation of the gradient function using the cell below."
      ],
      "metadata": {
        "id": "C545kQeM_igr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_tmp = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\n",
        "y_tmp = np.array([0, 0, 0, 1, 1, 1])\n",
        "w_tmp = np.array([2.,3.])\n",
        "b_tmp = 1.\n",
        "dj_db_tmp, dj_dw_tmp = compute_gradient_logistic(X_tmp, y_tmp, w_tmp, b_tmp)\n",
        "print(f\"dj_db: {dj_db_tmp}\" )\n",
        "print(f\"dj_dw: {dj_dw_tmp.tolist()}\" )"
      ],
      "metadata": {
        "id": "DXoBtGBp_jsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Descent Code\n",
        "The code implementing equation (1) above is implemented below. Take a moment to locate and compare the functions in the routine to the equations above"
      ],
      "metadata": {
        "id": "XJrHcpyS_vVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, y, w_in, b_in, alpha, num_iters): \n",
        "    \"\"\"\n",
        "    Performs batch gradient descent\n",
        "    \n",
        "    Args:\n",
        "      X (ndarray (m,n)   : Data, m examples with n features\n",
        "      y (ndarray (m,))   : target values\n",
        "      w_in (ndarray (n,)): Initial values of model parameters  \n",
        "      b_in (scalar)      : Initial values of model parameter\n",
        "      alpha (float)      : Learning rate\n",
        "      num_iters (scalar) : number of iterations to run gradient descent\n",
        "      \n",
        "    Returns:\n",
        "      w (ndarray (n,))   : Updated values of parameters\n",
        "      b (scalar)         : Updated value of parameter \n",
        "    \"\"\"\n",
        "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
        "    J_history = []\n",
        "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
        "    b = b_in\n",
        "    \n",
        "    for i in range(num_iters):\n",
        "        # Calculate the gradient and update the parameters\n",
        "        dj_db, dj_dw = compute_gradient_logistic(X, y, w, b)   \n",
        "\n",
        "        # Update Parameters using w, b, alpha and gradient\n",
        "        w = w - alpha * dj_dw               \n",
        "        b = b - alpha * dj_db               \n",
        "      \n",
        "        # Save cost J at each iteration\n",
        "        if i<100000:      # prevent resource exhaustion \n",
        "            J_history.append( compute_cost_logistic(X, y, w, b) )\n",
        "\n",
        "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
        "        if i% math.ceil(num_iters / 10) == 0:\n",
        "            print(f\"Iteration {i:4d}: Cost {J_history[-1]}   \")\n",
        "        \n",
        "    return w, b, J_history         #return final w,b and J history for graphing\n"
      ],
      "metadata": {
        "id": "J0NC2WY0_vlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's run gradient descent on our data set."
      ],
      "metadata": {
        "id": "oSjrDq9u_ybj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w_tmp  = np.zeros_like(X_train[0])\n",
        "b_tmp  = 0.\n",
        "alph = 0.1\n",
        "iters = 10000\n",
        "\n",
        "w_out, b_out, _ = gradient_descent(X_train, y_train, w_tmp, b_tmp, alph, iters) \n",
        "print(f\"\\nupdated parameters: w:{w_out}, b:{b_out}\")"
      ],
      "metadata": {
        "id": "larlLuvGACM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's plot the results of gradient descent:"
      ],
      "metadata": {
        "id": "duwrSmAqALG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig,ax = plt.subplots(1,1,figsize=(5,4))\n",
        "# plot the probability \n",
        "plt_prob(ax, w_out, b_out)\n",
        "\n",
        "# Plot the original data\n",
        "ax.set_ylabel(r'$x_1$')\n",
        "ax.set_xlabel(r'$x_0$')   \n",
        "ax.axis([0, 4, 0, 3.5])\n",
        "plot_data(X_train,y_train,ax)\n",
        "\n",
        "# Plot the decision boundary\n",
        "x0 = -b_out/w_out[0]\n",
        "x1 = -b_out/w_out[1]\n",
        "ax.plot([0,x0],[x1,0], c=dlc[\"dlblue\"], lw=1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Kxizl2jxALfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the plot above:\n",
        "\n",
        "the shading reflects the probability y=1 (result prior to decision boundary)\n",
        "the decision boundary is the line at which the probability = 0.5"
      ],
      "metadata": {
        "id": "OeHgT-7VAP5x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another Data set\n",
        "Let's return to a one-variable data set. With just two parameters,  𝑤 ,  𝑏 , it is possible to plot the cost function using a contour plot to get a better idea of what gradient descent is up to."
      ],
      "metadata": {
        "id": "XE1wATVhARUj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = np.array([0., 1, 2, 3, 4, 5])\n",
        "y_train = np.array([0,  0, 0, 1, 1, 1])"
      ],
      "metadata": {
        "id": "U1tIkD2UAQOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As before, we'll use a helper function to plot this data. The data points with label  𝑦=1  are shown as red crosses, while the data points with label  𝑦=0  are shown as blue circles."
      ],
      "metadata": {
        "id": "K99WKeMoAVjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig,ax = plt.subplots(1,1,figsize=(4,3))\n",
        "plt_tumor_data(x_train, y_train, ax)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dqFODfd4AW29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the plot below, try:\n",
        "\n",
        "changing  𝑤  and  𝑏  by clicking within the contour plot on the upper right.\n",
        "changes may take a second or two\n",
        "note the changing value of cost on the upper left plot.\n",
        "note the cost is accumulated by a loss on each example (vertical dotted lines)\n",
        "run gradient descent by clicking the orange button.\n",
        "note the steadily decreasing cost (contour and cost plot are in log(cost)\n",
        "clicking in the contour plot will reset the model for a new run\n",
        "to reset the plot, rerun the cell"
      ],
      "metadata": {
        "id": "IWiA5jPvAcMw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w_range = np.array([-1, 7])\n",
        "b_range = np.array([1, -14])\n",
        "quad = plt_quad_logistic( x_train, y_train, w_range, b_range )"
      ],
      "metadata": {
        "id": "jWY8DDkTAciF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}