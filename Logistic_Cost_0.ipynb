{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM/v6mHPT/qQshrDlBN5eOM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rgspatial/Practicing/blob/main/Logistic_Cost_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "%matplotlib widget\n",
        "import matplotlib.pyplot as plt\n",
        "from plt_logistic_loss import  plt_logistic_cost, plt_two_logistic_loss_curves, plt_simple_example\n",
        "from plt_logistic_loss import soup_bowl, plt_logistic_squared_error\n",
        "plt.style.use('./deeplearning.mplstyle')"
      ],
      "metadata": {
        "id": "QEbjNTfNfn1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soup_bowl()"
      ],
      "metadata": {
        "id": "89xqeEhhf0yj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = np.array([0., 1, 2, 3, 4, 5],dtype=np.longdouble)\n",
        "y_train = np.array([0,  0, 0, 1, 1, 1],dtype=np.longdouble)\n",
        "plt_simple_example(x_train, y_train)"
      ],
      "metadata": {
        "id": "Ms66rGUxf0vg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic regression requires a cost function more suitable to its non-linear nature. This starts with a Loss function. This is described below.\n",
        "\n",
        "Logistic Loss Function\n",
        "\n",
        "Logistic Regression uses a loss function more suited to the task of categorization where the target is 0 or 1 rather than any number.\n",
        "\n",
        "Definition Note: In this course, these definitions are used:\n",
        "Loss is a measure of the difference of a single example to its target value while the\n",
        "Cost is a measure of the losses over the training set\n",
        "\n",
        "This is defined:\n",
        "\n",
        "𝑙𝑜𝑠𝑠(𝑓𝐰,𝑏(𝐱(𝑖)),𝑦(𝑖)) is the cost for a single data point, which is:\n",
        "𝑙𝑜𝑠𝑠(𝑓𝐰,𝑏(𝐱(𝑖)),𝑦(𝑖))={−log(𝑓𝐰,𝑏(𝐱(𝑖)))−log(1−𝑓𝐰,𝑏(𝐱(𝑖)))if 𝑦(𝑖)=1if 𝑦(𝑖)=0\n",
        "𝑓𝐰,𝑏(𝐱(𝑖)) is the model's prediction, while 𝑦(𝑖) is the target value.\n",
        "\n",
        "𝑓𝐰,𝑏(𝐱(𝑖))=𝑔(𝐰⋅𝐱(𝑖)+𝑏) where function 𝑔 is the sigmoid function.\n",
        "\n",
        "The defining feature of this loss function is the fact that it uses two separate curves. One for the case when the target is zero or (𝑦=0) and another for when the target is one (𝑦=1). Combined, these curves provide the behavior useful for a loss function, namely, being zero when the prediction matches the target and rapidly increasing in value as the prediction differs from the target. Consider the curves below:"
      ],
      "metadata": {
        "id": "SZvGKk1eizFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt_two_logistic_loss_curves()"
      ],
      "metadata": {
        "id": "fuhb-0hmf0tF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combined, the curves are similar to the quadratic curve of the squared error loss. Note, the x-axis is  𝑓𝐰,𝑏  which is the output of a sigmoid. The sigmoid output is strictly between 0 and 1.\n",
        "\n",
        "The loss function above can be rewritten to be easier to implement.\n",
        "𝑙𝑜𝑠𝑠(𝑓𝐰,𝑏(𝐱(𝑖)),𝑦(𝑖))=(−𝑦(𝑖)log(𝑓𝐰,𝑏(𝐱(𝑖)))−(1−𝑦(𝑖))log(1−𝑓𝐰,𝑏(𝐱(𝑖)))\n",
        "This is a rather formidable-looking equation. It is less daunting when you consider 𝑦(𝑖) can have only two values, 0 and 1. One can then consider the equation in two pieces:\n",
        "when 𝑦(𝑖)=0, the left-hand term is eliminated:\n",
        "𝑙𝑜𝑠𝑠(𝑓𝐰,𝑏(𝐱(𝑖)),0)=(−(0)log(𝑓𝐰,𝑏(𝐱(𝑖)))−(1−0)log(1−𝑓𝐰,𝑏(𝐱(𝑖)))=−log(1−𝑓𝐰,𝑏(𝐱(𝑖)))\n",
        "and when 𝑦(𝑖)=1, the right-hand term is eliminated:\n",
        "𝑙𝑜𝑠𝑠(𝑓𝐰,𝑏(𝐱(𝑖)),1)=(−(1)log(𝑓𝐰,𝑏(𝐱(𝑖)))−(1−1)log(1−𝑓𝐰,𝑏(𝐱(𝑖)))=−log(𝑓𝐰,𝑏(𝐱(𝑖)))\n",
        "OK, with this new logistic loss function, a cost function can be produced that incorporates the loss from all the examples. This will be the topic of the next lab. For now, let's take a look at the cost vs parameters curve for the simple example we considered above:"
      ],
      "metadata": {
        "id": "yGYuZ0zNjAaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.close('all')\n",
        "cst = plt_logistic_cost(x_train,y_train)"
      ],
      "metadata": {
        "id": "43usdpY4f0qQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This curve is well suited to gradient descent! It does not have plateaus, local minima, or discontinuities. Note, it is not a bowl as in the case of squared error. Both the cost and the log of the cost are plotted to illuminate the fact that the curve, when the cost is small, has a slope and continues to decline."
      ],
      "metadata": {
        "id": "jZX37Q3vjRtr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S4rHU4G_f0kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5rvzooqif0h6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Houj1ePuf0fT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}