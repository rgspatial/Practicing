{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMu1VHcjHDmNZdIb15YaPDq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rgspatial/Practicing/blob/main/Cost_Function_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iP0lNS5AyoSh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = np.array([1.0, 2.0])           #(size in 1000 square feet)\n",
        "y_train = np.array([300.0, 500.0])           #(price in 1000s of dollars)"
      ],
      "metadata": {
        "id": "76UjHIqQphK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost(x, y, w, b): \n",
        "    \"\"\"\n",
        "    Computes the cost function for linear regression.\n",
        "    \n",
        "    Args:\n",
        "      x (ndarray (m,)): Data, m examples \n",
        "      y (ndarray (m,)): target values\n",
        "      w,b (scalar)    : model parameters  \n",
        "    \n",
        "    Returns\n",
        "        total_cost (float): The cost of using w,b as the parameters for linear regression\n",
        "               to fit the data points in x and y\n",
        "    \"\"\"\n",
        "    # number of training examples\n",
        "    m = x.shape[0] \n",
        "    \n",
        "    cost_sum = 0 \n",
        "    for i in range(m): \n",
        "        f_wb = w * x[i] + b   \n",
        "        cost = (f_wb - y[i]) ** 2  \n",
        "        cost_sum = cost_sum + cost  \n",
        "    total_cost = (1 / (2 * m)) * cost_sum  \n",
        "\n",
        "    return total_cost"
      ],
      "metadata": {
        "id": "PzUOXjiOrIxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient(x, y, w, b): \n",
        "    \"\"\"\n",
        "    Computes the gradient for linear regression \n",
        "    Args:\n",
        "      x (ndarray): Shape (m,) Input to the model (Population of cities) \n",
        "      y (ndarray): Shape (m,) Label (Actual profits for the cities)\n",
        "      w, b (scalar): Parameters of the model  \n",
        "    Returns\n",
        "      dj_dw (scalar): The gradient of the cost w.r.t. the parameters w\n",
        "      dj_db (scalar): The gradient of the cost w.r.t. the parameter b     \n",
        "     \"\"\"\n",
        "    \n",
        "    # Number of training examples\n",
        "    m = x.shape[0]\n",
        "    \n",
        "    # You need to return the following variables correctly\n",
        "    dj_dw = 0\n",
        "    dj_db = 0\n",
        "    \n",
        "    ### START CODE HERE ### \n",
        "    for i in range(m):\n",
        "        f_wb = w * x[i] + b \n",
        "        dj_dw_i = (f_wb - y[i]) * x[i]\n",
        "        dj_db_i = f_wb - y[i]\n",
        "        dj_db += dj_db_i\n",
        "        dj_dw += dj_dw_i\n",
        "        \n",
        "    dj_dw = dj_dw / m\n",
        "    dj_db = dj_db / m\n",
        "\n",
        "    ### END CODE HERE ### \n",
        "        \n",
        "    return dj_dw, dj_db"
      ],
      "metadata": {
        "id": "Pv_JkIpTrQV_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute and display gradient with w initialized to zeroes\n",
        "initial_w = 0\n",
        "initial_b = 0\n",
        "\n",
        "tmp_dj_dw, tmp_dj_db = compute_gradient(x_train, y_train, initial_w, initial_b)\n",
        "print('Gradient at initial w, b (zeros):', tmp_dj_dw, tmp_dj_db)\n",
        "\n",
        "compute_gradient_test(compute_gradient)"
      ],
      "metadata": {
        "id": "WT6vTuzmZXIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute and display cost and gradient with non-zero w\n",
        "test_w = 0.2\n",
        "test_b = 0.2\n",
        "tmp_dj_dw, tmp_dj_db = compute_gradient(x_train, y_train, test_w, test_b)\n",
        "\n",
        "print('Gradient at test w, b:', tmp_dj_dw, tmp_dj_db)"
      ],
      "metadata": {
        "id": "zL0nh4olZZ2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(x, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters): \n",
        "    \"\"\"\n",
        "    Performs batch gradient descent to learn theta. Updates theta by taking \n",
        "    num_iters gradient steps with learning rate alpha\n",
        "    \n",
        "    Args:\n",
        "      x :    (ndarray): Shape (m,)\n",
        "      y :    (ndarray): Shape (m,)\n",
        "      w_in, b_in : (scalar) Initial values of parameters of the model\n",
        "      cost_function: function to compute cost\n",
        "      gradient_function: function to compute the gradient\n",
        "      alpha : (float) Learning rate\n",
        "      num_iters : (int) number of iterations to run gradient descent\n",
        "    Returns\n",
        "      w : (ndarray): Shape (1,) Updated values of parameters of the model after\n",
        "          running gradient descent\n",
        "      b : (scalar)                Updated value of parameter of the model after\n",
        "          running gradient descent\n",
        "    \"\"\"\n",
        "    \n",
        "    # number of training examples\n",
        "    m = len(x)\n",
        "    \n",
        "    # An array to store cost J and w's at each iteration â€” primarily for graphing later\n",
        "    J_history = []\n",
        "    w_history = []\n",
        "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
        "    b = b_in\n",
        "    \n",
        "    for i in range(num_iters):\n",
        "\n",
        "        # Calculate the gradient and update the parameters\n",
        "        dj_dw, dj_db = gradient_function(x, y, w, b )  \n",
        "\n",
        "        # Update Parameters using w, b, alpha and gradient\n",
        "        w = w - alpha * dj_dw               \n",
        "        b = b - alpha * dj_db               \n",
        "\n",
        "        # Save cost J at each iteration\n",
        "        if i<100000:      # prevent resource exhaustion \n",
        "            cost =  cost_function(x, y, w, b)\n",
        "            J_history.append(cost)\n",
        "\n",
        "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
        "        if i% math.ceil(num_iters/10) == 0:\n",
        "            w_history.append(w)\n",
        "            print(f\"Iteration {i:4}: Cost {float(J_history[-1]):8.2f}   \")\n",
        "        \n",
        "    return w, b, J_history, w_history #return w and J,w history for graphing"
      ],
      "metadata": {
        "id": "t-hFx31BZYTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize fitting parameters. Recall that the shape of w is (n,)\n",
        "initial_w = 0.\n",
        "initial_b = 0.\n",
        "\n",
        "# some gradient descent settings\n",
        "iterations = 1500\n",
        "alpha = 0.01\n",
        "\n",
        "w,b,_,_ = gradient_descent(x_train ,y_train, initial_w, initial_b, \n",
        "                     compute_cost, compute_gradient, alpha, iterations)\n",
        "print(\"w,b found by gradient descent:\", w, b)"
      ],
      "metadata": {
        "id": "77DdyxWoaEUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = x_train.shape[0]\n",
        "predicted = np.zeros(m)\n",
        "\n",
        "for i in range(m):\n",
        "    predicted[i] = w * x_train[i] + b"
      ],
      "metadata": {
        "id": "6MYEdUZwaKjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the linear fit\n",
        "plt.plot(x_train, predicted, c = \"b\")\n",
        "\n",
        "# Create a scatter plot of the data. \n",
        "plt.scatter(x_train, y_train, marker='x', c='r') \n",
        "\n",
        "# Set the title\n",
        "plt.title(\"Profits vs. Population per city\")\n",
        "# Set the y-axis label\n",
        "plt.ylabel('Profit in $10,000')\n",
        "# Set the x-axis label\n",
        "plt.xlabel('Population of City in 10,000s')"
      ],
      "metadata": {
        "id": "GGtkJ8naaNAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict1 = 3.5 * w + b\n",
        "print('For population = 35,000, we predict a profit of $%.2f' % (predict1*10000))\n",
        "\n",
        "predict2 = 7.0 * w + b\n",
        "print('For population = 70,000, we predict a profit of $%.2f' % (predict2*10000))"
      ],
      "metadata": {
        "id": "rG8wbKWAaUAr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}