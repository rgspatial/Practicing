{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPjqprtwzXmzBgcXwoQgjuU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rgspatial/Practicing/blob/main/Logistic_Regression_Cost_Function_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-bfupEijbwU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "%matplotlib widget\n",
        "import matplotlib.pyplot as plt\n",
        "from lab_utils_common import  plot_data, sigmoid, dlc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset\n",
        "X_train = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])  #(m,n)\n",
        "y_train = np.array([0, 0, 0, 1, 1, 1])                                           #(m,)"
      ],
      "metadata": {
        "id": "GOHADDTQlTjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use a helper function to plot this data. The data points with label  ğ‘¦=1  are shown as red crosses, while the data points with label  ğ‘¦=0  are shown as blue circles."
      ],
      "metadata": {
        "id": "A7PeMN6dlXWn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig,ax = plt.subplots(1,1,figsize=(4,4))\n",
        "plot_data(X_train, y_train, ax)\n",
        "\n",
        "# Set both axes to be from 0-4\n",
        "ax.axis([0, 4, 0, 3.5])\n",
        "ax.set_ylabel('$x_1$', fontsize=12)\n",
        "ax.set_xlabel('$x_0$', fontsize=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1_-TRTMdlX-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cost function\n",
        "In a previous lab, you developed the logistic loss function. Recall, loss is defined to apply to one example. Here you combine the losses to form the cost, which includes all the examples.\n",
        "\n",
        "Recall that for logistic regression, the cost function is of the form\n",
        "\n",
        "ğ½(ğ°,ğ‘)=1ğ‘šâˆ‘ğ‘–=0ğ‘šâˆ’1[ğ‘™ğ‘œğ‘ ğ‘ (ğ‘“ğ°,ğ‘(ğ±(ğ‘–)),ğ‘¦(ğ‘–))](1)\n",
        "where\n",
        "\n",
        "ğ‘™ğ‘œğ‘ ğ‘ (ğ‘“ğ°,ğ‘(ğ±(ğ‘–)),ğ‘¦(ğ‘–))  is the cost for a single data point, which is:\n",
        "\n",
        "ğ‘™ğ‘œğ‘ ğ‘ (ğ‘“ğ°,ğ‘(ğ±(ğ‘–)),ğ‘¦(ğ‘–))=âˆ’ğ‘¦(ğ‘–)log(ğ‘“ğ°,ğ‘(ğ±(ğ‘–)))âˆ’(1âˆ’ğ‘¦(ğ‘–))log(1âˆ’ğ‘“ğ°,ğ‘(ğ±(ğ‘–)))(2)\n",
        "where m is the number of training examples in the data set and:\n",
        "ğ‘“ğ°,ğ‘(ğ±(ğ¢))ğ‘§(ğ‘–)ğ‘”(ğ‘§(ğ‘–))=ğ‘”(ğ‘§(ğ‘–))=ğ°â‹…ğ±(ğ‘–)+ğ‘=11+ğ‘’âˆ’ğ‘§(ğ‘–)(3)(4)(5)"
      ],
      "metadata": {
        "id": "gTT08IR7leJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The algorithm for compute_cost_logistic loops over all the examples calculating the loss for each example and accumulating the total.\n",
        "# Note that the variables X and y are not scalar values but matrices of shape ( ğ‘š,ğ‘› ) and ( ğ‘š ,) respectively, \n",
        "# where  ğ‘›  is the number of features and  ğ‘š  is the number of training examples.\n",
        "\n",
        "def compute_cost_logistic(X, y, w, b):\n",
        "    \"\"\"\n",
        "    Computes cost\n",
        "\n",
        "    Args:\n",
        "      X (ndarray (m,n)): Data, m examples with n features\n",
        "      y (ndarray (m,)) : target values\n",
        "      w (ndarray (n,)) : model parameters  \n",
        "      b (scalar)       : model parameter\n",
        "      \n",
        "    Returns:\n",
        "      cost (scalar): cost\n",
        "    \"\"\"\n",
        "\n",
        "    m = X.shape[0]\n",
        "    cost = 0.0\n",
        "    for i in range(m):\n",
        "        z_i = np.dot(X[i],w) + b\n",
        "        f_wb_i = sigmoid(z_i)\n",
        "        cost +=  -y[i]*np.log(f_wb_i) - (1-y[i])*np.log(1-f_wb_i)\n",
        "             \n",
        "    cost = cost / m\n",
        "    return cost"
      ],
      "metadata": {
        "id": "QtPQDOSDleZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the implementation of the cost function using the cell below.\n",
        "w_tmp = np.array([1,1])\n",
        "b_tmp = -3\n",
        "print(compute_cost_logistic(X_train, y_train, w_tmp, b_tmp))"
      ],
      "metadata": {
        "id": "rVhsk-0Olzlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example\n",
        "Now, let's see what the cost function output is for a different value of  ğ‘¤ .\n",
        "\n",
        "In a previous lab, you plotted the decision boundary for  ğ‘=âˆ’3,ğ‘¤0=1,ğ‘¤1=1 . That is, you had b = -3, w = np.array([1,1]).\n",
        "\n",
        "Let's say you want to see if  ğ‘=âˆ’4,ğ‘¤0=1,ğ‘¤1=1 , or b = -4, w = np.array([1,1]) provides a better model.\n",
        "\n",
        "Let's first plot the decision boundary for these two different  ğ‘  values to see which one fits the data better.\n",
        "\n",
        "For  ğ‘=âˆ’3,ğ‘¤0=1,ğ‘¤1=1 , we'll plot  âˆ’3+ğ‘¥0+ğ‘¥1=0  (shown in blue)\n",
        "For  ğ‘=âˆ’4,ğ‘¤0=1,ğ‘¤1=1 , we'll plot  âˆ’4+ğ‘¥0+ğ‘¥1=0  (shown in magenta)"
      ],
      "metadata": {
        "id": "FbEgaCT-l3PD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Choose values between 0 and 6\n",
        "x0 = np.arange(0,6)\n",
        "\n",
        "# Plot the two decision boundaries\n",
        "x1 = 3 - x0\n",
        "x1_other = 4 - x0\n",
        "\n",
        "fig,ax = plt.subplots(1, 1, figsize=(4,4))\n",
        "# Plot the decision boundary\n",
        "ax.plot(x0,x1, c=dlc[\"dlblue\"], label=\"$b$=-3\")\n",
        "ax.plot(x0,x1_other, c=dlc[\"dlmagenta\"], label=\"$b$=-4\")\n",
        "ax.axis([0, 4, 0, 4])\n",
        "\n",
        "# Plot the original data\n",
        "plot_data(X_train,y_train,ax)\n",
        "ax.axis([0, 4, 0, 4])\n",
        "ax.set_ylabel('$x_1$', fontsize=12)\n",
        "ax.set_xlabel('$x_0$', fontsize=12)\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.title(\"Decision Boundary\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3fkeedLVl3fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see from this plot that b = -4, w = np.array([1,1]) is a worse model for the training data. Let's see if the cost function implementation reflects this."
      ],
      "metadata": {
        "id": "XKeXS7kkmAH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w_array1 = np.array([1,1])\n",
        "b_1 = -3\n",
        "w_array2 = np.array([1,1])\n",
        "b_2 = -4\n",
        "\n",
        "print(\"Cost for b = -3 : \", compute_cost_logistic(X_train, y_train, w_array1, b_1))\n",
        "print(\"Cost for b = -4 : \", compute_cost_logistic(X_train, y_train, w_array2, b_2))"
      ],
      "metadata": {
        "id": "TDTtrf62mAZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected output\n",
        "\n",
        "Cost for b = -3 : 0.3668667864055175\n",
        "\n",
        "Cost for b = -4 : 0.5036808636748461\n",
        "\n",
        "You can see the cost function behaves as expected and the cost for b = -4, w = np.array([1,1]) is indeed higher than the cost for b = -3, w = np.array([1,1])"
      ],
      "metadata": {
        "id": "Hp7g8-PxmE8B"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K423QsF2mFV9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}