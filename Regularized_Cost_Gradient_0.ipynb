{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO1tkm7O6IeQ5ltgg0K3gzd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rgspatial/Practicing/blob/main/Regularized_Cost_Gradient_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuJeTkTTO_Q5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "%matplotlib widget\n",
        "import matplotlib.pyplot as plt\n",
        "from plt_overfit import overfit_example, output\n",
        "from lab_utils_common import sigmoid\n",
        "np.set_printoptions(precision=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cost\n",
        "The cost functions differ significantly between linear and logistic regression, but adding regularization to the equations is the same.\n",
        "Gradient\n",
        "The gradient functions for linear and logistic regression are very similar. They differ only in the implementation of  ğ‘“ğ‘¤ğ‘ ."
      ],
      "metadata": {
        "id": "nLEqRwYUPIkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost_linear_reg(X, y, w, b, lambda_ = 1):\n",
        "    \"\"\"\n",
        "    Computes the cost over all examples\n",
        "    Args:\n",
        "      X (ndarray (m,n): Data, m examples with n features\n",
        "      y (ndarray (m,)): target values\n",
        "      w (ndarray (n,)): model parameters  \n",
        "      b (scalar)      : model parameter\n",
        "      lambda_ (scalar): Controls amount of regularization\n",
        "    Returns:\n",
        "      total_cost (scalar):  cost \n",
        "    \"\"\"\n",
        "\n",
        "    m  = X.shape[0]\n",
        "    n  = len(w)\n",
        "    cost = 0.\n",
        "    for i in range(m):\n",
        "        f_wb_i = np.dot(X[i], w) + b                                   #(n,)(n,)=scalar, see np.dot\n",
        "        cost = cost + (f_wb_i - y[i])**2                               #scalar             \n",
        "    cost = cost / (2 * m)                                              #scalar  \n",
        " \n",
        "    reg_cost = 0\n",
        "    for j in range(n):\n",
        "        reg_cost += (w[j]**2)                                          #scalar\n",
        "    reg_cost = (lambda_/(2*m)) * reg_cost                              #scalar\n",
        "    \n",
        "    total_cost = cost + reg_cost                                       #scalar\n",
        "    return total_cost                                                  #scalar"
      ],
      "metadata": {
        "id": "OcH5xEW5PJDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1)\n",
        "X_tmp = np.random.rand(5,6)\n",
        "y_tmp = np.array([0,1,0,1,0])\n",
        "w_tmp = np.random.rand(X_tmp.shape[1]).reshape(-1,)-0.5\n",
        "b_tmp = 0.5\n",
        "lambda_tmp = 0.7\n",
        "cost_tmp = compute_cost_linear_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n",
        "\n",
        "print(\"Regularized cost:\", cost_tmp)"
      ],
      "metadata": {
        "id": "_ap0LAFJPPRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost_logistic_reg(X, y, w, b, lambda_ = 1):\n",
        "    \"\"\"\n",
        "    Computes the cost over all examples\n",
        "    Args:\n",
        "    Args:\n",
        "      X (ndarray (m,n): Data, m examples with n features\n",
        "      y (ndarray (m,)): target values\n",
        "      w (ndarray (n,)): model parameters  \n",
        "      b (scalar)      : model parameter\n",
        "      lambda_ (scalar): Controls amount of regularization\n",
        "    Returns:\n",
        "      total_cost (scalar):  cost \n",
        "    \"\"\"\n",
        "\n",
        "    m,n  = X.shape\n",
        "    cost = 0.\n",
        "    for i in range(m):\n",
        "        z_i = np.dot(X[i], w) + b                                      #(n,)(n,)=scalar, see np.dot\n",
        "        f_wb_i = sigmoid(z_i)                                          #scalar\n",
        "        cost +=  -y[i]*np.log(f_wb_i) - (1-y[i])*np.log(1-f_wb_i)      #scalar\n",
        "             \n",
        "    cost = cost/m                                                      #scalar\n",
        "\n",
        "    reg_cost = 0\n",
        "    for j in range(n):\n",
        "        reg_cost += (w[j]**2)                                          #scalar\n",
        "    reg_cost = (lambda_/(2*m)) * reg_cost                              #scalar\n",
        "    \n",
        "    total_cost = cost + reg_cost                                       #scalar\n",
        "    return total_cost                                                  #scalar"
      ],
      "metadata": {
        "id": "_DNM_H-ZPTuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# see in action\n",
        "np.random.seed(1)\n",
        "X_tmp = np.random.rand(5,6)\n",
        "y_tmp = np.array([0,1,0,1,0])\n",
        "w_tmp = np.random.rand(X_tmp.shape[1]).reshape(-1,)-0.5\n",
        "b_tmp = 0.5\n",
        "lambda_tmp = 0.7\n",
        "cost_tmp = compute_cost_logistic_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n",
        "\n",
        "print(\"Regularized cost:\", cost_tmp)"
      ],
      "metadata": {
        "id": "oBefg7k2PVnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient descent with regularization\n",
        "The basic algorithm for running gradient descent does not change with regularization, it is:\n",
        "repeat until convergence:{ğ‘¤ğ‘—=ğ‘¤ğ‘—âˆ’ğ›¼âˆ‚ğ½(ğ°,ğ‘)âˆ‚ğ‘¤ğ‘—ğ‘=ğ‘âˆ’ğ›¼âˆ‚ğ½(ğ°,ğ‘)âˆ‚ğ‘}for j := 0..n-1(1)\n",
        "Where each iteration performs simultaneous updates on  ğ‘¤ğ‘—  for all  ğ‘— .\n",
        "\n",
        "What changes with regularization is computing the gradients.\n",
        "\n",
        "Computing the Gradient with regularization (both linear/logistic)\n",
        "The gradient calculation for both linear and logistic regression are nearly identical, differing only in computation of ğ‘“ğ°ğ‘.\n",
        "âˆ‚ğ½(ğ°,ğ‘)âˆ‚ğ‘¤ğ‘—âˆ‚ğ½(ğ°,ğ‘)âˆ‚ğ‘=1ğ‘šâˆ‘ğ‘–=0ğ‘šâˆ’1(ğ‘“ğ°,ğ‘(ğ±(ğ‘–))âˆ’ğ‘¦(ğ‘–))ğ‘¥(ğ‘–)ğ‘—+ğœ†ğ‘šğ‘¤ğ‘—=1ğ‘šâˆ‘ğ‘–=0ğ‘šâˆ’1(ğ‘“ğ°,ğ‘(ğ±(ğ‘–))âˆ’ğ‘¦(ğ‘–))(2)(3)\n",
        "m is the number of training examples in the data set\n",
        "ğ‘“ğ°,ğ‘(ğ‘¥(ğ‘–)) is the model's prediction, while ğ‘¦(ğ‘–) is the target\n",
        "For a linear regression model\n",
        "ğ‘“ğ°,ğ‘(ğ‘¥)=ğ°â‹…ğ±+ğ‘\n",
        "For a logistic regression model\n",
        "ğ‘§=ğ°â‹…ğ±+ğ‘\n",
        "ğ‘“ğ°,ğ‘(ğ‘¥)=ğ‘”(ğ‘§)\n",
        "where ğ‘”(ğ‘§) is the sigmoid function:\n",
        "ğ‘”(ğ‘§)=11+ğ‘’âˆ’ğ‘§\n",
        "The term which adds regularization is the ğœ†ğ‘šğ‘¤ğ‘—."
      ],
      "metadata": {
        "id": "Aahwmbt2PckG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient function for regularized linear regression\n",
        "\n",
        "def compute_gradient_linear_reg(X, y, w, b, lambda_): \n",
        "    \"\"\"\n",
        "    Computes the gradient for linear regression \n",
        "    Args:\n",
        "      X (ndarray (m,n): Data, m examples with n features\n",
        "      y (ndarray (m,)): target values\n",
        "      w (ndarray (n,)): model parameters  \n",
        "      b (scalar)      : model parameter\n",
        "      lambda_ (scalar): Controls amount of regularization\n",
        "      \n",
        "    Returns:\n",
        "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
        "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. \n",
        "    \"\"\"\n",
        "    m,n = X.shape           #(number of examples, number of features)\n",
        "    dj_dw = np.zeros((n,))\n",
        "    dj_db = 0.\n",
        "\n",
        "    for i in range(m):                             \n",
        "        err = (np.dot(X[i], w) + b) - y[i]                 \n",
        "        for j in range(n):                         \n",
        "            dj_dw[j] = dj_dw[j] + err * X[i, j]               \n",
        "        dj_db = dj_db + err                        \n",
        "    dj_dw = dj_dw / m                                \n",
        "    dj_db = dj_db / m   \n",
        "    \n",
        "    for j in range(n):\n",
        "        dj_dw[j] = dj_dw[j] + (lambda_/m) * w[j]\n",
        "\n",
        "    return dj_db, dj_dw"
      ],
      "metadata": {
        "id": "4ZPytBaPPc5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# see in action\n",
        "\n",
        "np.random.seed(1)\n",
        "X_tmp = np.random.rand(5,3)\n",
        "y_tmp = np.array([0,1,0,1,0])\n",
        "w_tmp = np.random.rand(X_tmp.shape[1])\n",
        "b_tmp = 0.5\n",
        "lambda_tmp = 0.7\n",
        "dj_db_tmp, dj_dw_tmp =  compute_gradient_linear_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n",
        "\n",
        "print(f\"dj_db: {dj_db_tmp}\", )\n",
        "print(f\"Regularized dj_dw:\\n {dj_dw_tmp.tolist()}\", )"
      ],
      "metadata": {
        "id": "zaGMbxN2PnfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient function for regularized logistic regression\n",
        "\n",
        "def compute_gradient_logistic_reg(X, y, w, b, lambda_): \n",
        "    \"\"\"\n",
        "    Computes the gradient for linear regression \n",
        " \n",
        "    Args:\n",
        "      X (ndarray (m,n): Data, m examples with n features\n",
        "      y (ndarray (m,)): target values\n",
        "      w (ndarray (n,)): model parameters  \n",
        "      b (scalar)      : model parameter\n",
        "      lambda_ (scalar): Controls amount of regularization\n",
        "    Returns\n",
        "      dj_dw (ndarray Shape (n,)): The gradient of the cost w.r.t. the parameters w. \n",
        "      dj_db (scalar)            : The gradient of the cost w.r.t. the parameter b. \n",
        "    \"\"\"\n",
        "    m,n = X.shape\n",
        "    dj_dw = np.zeros((n,))                            #(n,)\n",
        "    dj_db = 0.0                                       #scalar\n",
        "\n",
        "    for i in range(m):\n",
        "        f_wb_i = sigmoid(np.dot(X[i],w) + b)          #(n,)(n,)=scalar\n",
        "        err_i  = f_wb_i  - y[i]                       #scalar\n",
        "        for j in range(n):\n",
        "            dj_dw[j] = dj_dw[j] + err_i * X[i,j]      #scalar\n",
        "        dj_db = dj_db + err_i\n",
        "    dj_dw = dj_dw/m                                   #(n,)\n",
        "    dj_db = dj_db/m                                   #scalar\n",
        "\n",
        "    for j in range(n):\n",
        "        dj_dw[j] = dj_dw[j] + (lambda_/m) * w[j]\n",
        "\n",
        "    return dj_db, dj_dw  "
      ],
      "metadata": {
        "id": "EqKMxcXcPr5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the cell below to see it in action.\n",
        "\n",
        "np.random.seed(1)\n",
        "X_tmp = np.random.rand(5,3)\n",
        "y_tmp = np.array([0,1,0,1,0])\n",
        "w_tmp = np.random.rand(X_tmp.shape[1])\n",
        "b_tmp = 0.5\n",
        "lambda_tmp = 0.7\n",
        "dj_db_tmp, dj_dw_tmp =  compute_gradient_logistic_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n",
        "\n",
        "print(f\"dj_db: {dj_db_tmp}\", )\n",
        "print(f\"Regularized dj_dw:\\n {dj_dw_tmp.tolist()}\", )"
      ],
      "metadata": {
        "id": "B4j9mFFIPv2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# overfitting example\n",
        "\n",
        "plt.close(\"all\")\n",
        "display(output)\n",
        "ofit = overfit_example(True)"
      ],
      "metadata": {
        "id": "va644QCaP2pg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}